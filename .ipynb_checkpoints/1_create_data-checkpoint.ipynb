{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "\n",
    "Recently the Chinese Police was on the first page of the most famous newspapers because they have succesfully detected a criminal in a pop concert with 60k people from security cameras using a facial recognition system. Here's a [link of the news](http://www.bbc.com/news/world-asia-china-43751276) if you missed it. This is not the first time that they used the same technology to catch criminals, but it is certainly quite amazing what this technology is able to achieve.\n",
    "\n",
    "## \"Where is Syd?\"\n",
    "\n",
    "In this project, we will use [Tensorflow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) to detect Syd. It turns out that Syd participated to several Sport events of the last years, mostly during the opening ceremony. Unfortunately, we were able to get only some images of him during the ceremonies... but don't worry, Transfer Learning comes to help. We will use a pre-trained SSD model on COCO dataset to find Syd. If the trained model we will be good enough we will be able to detect Syd in the next games and tell to your family & friends: 'Look there, it's Syd! I found him ;)'.\n",
    "\n",
    "![wanted Syd](https://raw.githubusercontent.com/floydhub/object-detection-template/master/images/wanted-syd.jpg)\n",
    "*Are you able to catch Syd in this image?*\n",
    "\n",
    "The project is structered around 3 notebooks that have to been run in this order:\n",
    "\n",
    "- `1_create_data` where we build a TFRecords dataset from the images with bounding box annotations,\n",
    "- `2_training` where we perform the training, evaluation and model exportation,\n",
    "- `3_prediction` where we evaluate the model on new data.\n",
    "\n",
    "#### Template Structure\n",
    "\n",
    "- `install/` contains the instruction to replicate the installation of Tensorflow Object Detection - API,\n",
    "- `models/` contains the pretrained `ssdlite_mobilenet_v2` model on COCO dataset,\n",
    "- `object_detection/` is the Tensorflow Object Detection framework,\n",
    "- `slim/`, the TF Object Detection module is built upon [TF-SLIM](https://github.com/tensorflow/models/tree/master/research/slim) an high-level API of TensorFlow (tensorflow.contrib.slim) for defining, training and evaluating complex models (expecially CNN models),\n",
    "- `tfrecors_data/` will contains the dataset in TFRecord format after the execution of the `1_create_data` notebook,\n",
    "- `trained_models` will contains the model trained for the task after the execution of the `2_training` notebook.\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "To not waste GPU time, run this notebook on a CPU instance, and switch on GPU only for the training step.\n",
    "\n",
    "## Part 0: Creating a TF Record dataset\n",
    "\n",
    "In this notebook we will convert the dataset (images with bounding box) to the TFRecord format. The TFRecord file format is a simple record-oriented binary format that many TensorFlow applications use for training data. This is the format reccomended by Tensorflow and also a smart way to store all the data we need for train and evaluate deep learning model. \n",
    "\n",
    "For more see the [Importing Data](https://www.tensorflow.org/programmers_guide/datasets) guide.\n",
    "\n",
    "For more info about Tensorflow Object Detection, please refer to [the official docs](https://github.com/tensorflow/models/tree/master/research/object_detection/g3doc).\n",
    "\n",
    "### Instructions\n",
    "- To execute a code cell, click on the cell and press `Shift + Enter`(shortcut for Run).\n",
    "- To learn more about Workspaces, check out the [Getting Started Notebook](./get_started_workspace.ipynb).\n",
    "- **Tip**: *Feel free to try this Notebook with your own data and on your own super awesome object detection task.\n",
    "\n",
    "Now, let's get started! ðŸš€\n",
    "\n",
    "### Initial Setup\n",
    "Let's start by importing the packages and loading the csv file from which get all the data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/yk/Documents/TensorFlow/object-detection-template/slim\")\n",
    "\n",
    "from support import draw_outline, draw_rect, draw_text, bb_hw, show_img, open_image\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from collections import namedtuple, OrderedDict\n",
    "from object_detection.utils import dataset_util\n",
    "from tqdm import tqdm\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import path\n",
    "import cv2\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "# Path to the CSV input\n",
    "CSV_INPUT = '/home/yk/Documents/TensorFlow/object-detection-template/annotations.csv'\n",
    "\n",
    "# Path to the image directory\n",
    "IMAGE_DIR = '/home/yk/Documents/TensorFlow/object-detection-template/images/'\n",
    "\n",
    "# Path to output TFRecord\n",
    "OUTPUT = '/home/yk/Documents/TensorFlow/object-detection-template/tfrecords_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>class</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17_554589502390719.jpg</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>250ar</td>\n",
       "      <td>37</td>\n",
       "      <td>170</td>\n",
       "      <td>313</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12_554412777652541.jpg</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>50000en</td>\n",
       "      <td>24</td>\n",
       "      <td>287</td>\n",
       "      <td>278</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15_554413995583069.jpg</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>1000ar</td>\n",
       "      <td>28</td>\n",
       "      <td>68</td>\n",
       "      <td>385</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18_554589775630124.jpg</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>500ar</td>\n",
       "      <td>68</td>\n",
       "      <td>181</td>\n",
       "      <td>342</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18_554589974344365.jpg</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>500ar</td>\n",
       "      <td>19</td>\n",
       "      <td>226</td>\n",
       "      <td>363</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17_554455765612118.jpg</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>250ar</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>390</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14_554413505000676.jpg</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>1000en</td>\n",
       "      <td>17</td>\n",
       "      <td>206</td>\n",
       "      <td>264</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16_554454896740551.jpg</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>250en</td>\n",
       "      <td>8</td>\n",
       "      <td>86</td>\n",
       "      <td>367</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15_554593876321521.jpg</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>1000ar</td>\n",
       "      <td>6</td>\n",
       "      <td>102</td>\n",
       "      <td>368</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12_554412554236534.jpg</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>50000en</td>\n",
       "      <td>13</td>\n",
       "      <td>137</td>\n",
       "      <td>360</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5_3954032649_554159549007164.jpg</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>5000en</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>332</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16_554454745791047.jpg</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>250en</td>\n",
       "      <td>27</td>\n",
       "      <td>203</td>\n",
       "      <td>342</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            filename  width  height    class  xmin  ymin  \\\n",
       "0             17_554589502390719.jpg    400     400    250ar    37   170   \n",
       "1             12_554412777652541.jpg    400     400  50000en    24   287   \n",
       "2             15_554413995583069.jpg    400     400   1000ar    28    68   \n",
       "3             18_554589775630124.jpg    400     400    500ar    68   181   \n",
       "4             18_554589974344365.jpg    400     400    500ar    19   226   \n",
       "5             17_554455765612118.jpg    400     400    250ar    31     2   \n",
       "6             14_554413505000676.jpg    400     400   1000en    17   206   \n",
       "7             16_554454896740551.jpg    400     400    250en     8    86   \n",
       "8             15_554593876321521.jpg    400     400   1000ar     6   102   \n",
       "9             12_554412554236534.jpg    400     400  50000en    13   137   \n",
       "10  5_3954032649_554159549007164.jpg    400     400   5000en    28    11   \n",
       "11            16_554454745791047.jpg    400     400    250en    27   203   \n",
       "\n",
       "    xmax  ymax  \n",
       "0    313   323  \n",
       "1    278   396  \n",
       "2    385   229  \n",
       "3    342   311  \n",
       "4    363   391  \n",
       "5    390   187  \n",
       "6    264   308  \n",
       "7    367   275  \n",
       "8    368   277  \n",
       "9    360   272  \n",
       "10   332   156  \n",
       "11   342   361  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(CSV_INPUT)\n",
    "                       \n",
    "data.head(n=12)                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output above, we have only a few samples available: 10 images for training and 2 for the evalaution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "\n",
    "Run the code Cell below and play with the slider to plot the related training image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5475cd1effc944be8f544487143e6dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='Show training sample', max=10, min=1), Output()), _dom_câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "from ipywidgets import widgets\n",
    "\n",
    "# Get the values from csv file\n",
    "images_list = data['filename'].values\n",
    "x_min_list = data['xmin'].values\n",
    "x_max_list = data['xmax'].values\n",
    "y_min_list = data['ymin'].values\n",
    "y_max_list = data['ymax'].values\n",
    "\n",
    "def show_sample(sample_n):\n",
    "    image_path = os.path.join(IMAGE_DIR,images_list[sample_n-1])\n",
    "    ymin, xmin, ymax, xmax = y_min_list[sample_n-1], x_min_list[sample_n-1], y_max_list[sample_n-1], x_max_list[sample_n-1]\n",
    "    im = open_image(image_path)\n",
    "    ax = show_img(im, figsize=(15,15))\n",
    "    ax.set_title(image_path)\n",
    "    b = bb_hw((ymin, xmin, ymax, xmax))\n",
    "    draw_rect(ax, b)\n",
    "    draw_text(ax, [xmin-10, ymin-10], 'syd')\n",
    "    \n",
    "interact(show_sample, sample_n=widgets.IntSlider(value=1, min=1, max=10, description='Show training sample'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Val split\n",
    "\n",
    "10 samples for training and 2 for evalauting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Train / Val split\n",
    "train_df = data.iloc[:10]\n",
    "eval_df = data.iloc[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_text_to_int(row_label):\n",
    "    \"\"\"Replace the label with an int\"\"\"\n",
    "    if row_label == '50000ar':\n",
    "        return 1\n",
    "    elif row_label == '50000en':\n",
    "        return 2\n",
    "    elif row_label == '25000ar':\n",
    "        return 3\n",
    "    elif row_label == '25000en':\n",
    "        return 4\n",
    "    elif row_label == '10000ar':\n",
    "        return 5\n",
    "    elif row_label == '10000en':\n",
    "        return 6\n",
    "    elif row_label == '5000ar':\n",
    "        return 7\n",
    "    elif row_label == '5000en':\n",
    "        return 8\n",
    "    elif row_label == '1000ar':\n",
    "        return 9\n",
    "    elif row_label == '1000en':\n",
    "        return 10\n",
    "    elif row_label == '500ar':\n",
    "        return 11\n",
    "    elif row_label == '500en':\n",
    "        return 12\n",
    "    elif row_label == '250ar':\n",
    "        return 13\n",
    "    elif row_label == '250en':\n",
    "        return 14\n",
    "    else:\n",
    "        None\n",
    "        \n",
    "def split(df, group):\n",
    "    \"\"\"For each images, return a data object with all the labels/bbox in the images\n",
    "    \n",
    "    e.g.\n",
    "    \n",
    "    [data(filename='1.jpg', object=  filename  width  height  class  xmin  ymin  xmax  ymax\n",
    "     0    1.jpg   2048    1251  syd   706   513   743   562),\n",
    "     data(filename='10.jpg', object=   filename  width  height  class  xmin  ymin  xmax  ymax\n",
    "     1    10.jpg   1600     980  syd   715   157   733   181\n",
    "     19   10.jpg   1600     980  syd   428    83   483   145),\n",
    "     ...\n",
    "     data(filename='9.jpg', object=   filename  width  height  class  xmin  ymin  xmax  ymax\n",
    "     17    9.jpg   1298     951  syd   231   735   261   769)]\n",
    "     \n",
    "    \"\"\"\n",
    "    data = namedtuple('data', ['filename', 'object'])\n",
    "    gb = df.groupby(group)\n",
    "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
    "\n",
    "def create_tf_example(group, path):\n",
    "    \"\"\"\n",
    "    From data group object to TFRecord file\n",
    "    \n",
    "    Note: we are handling JPG data format and bbox labels. \n",
    "    If you need to work on PNG data with mask or polygon labels, you will have to edit the code a bit.\n",
    "    \"\"\"\n",
    "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "    filename = group.filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "    for index, row in group.object.iterrows():\n",
    "        xmins.append(row['xmin'] / width)\n",
    "        xmaxs.append(row['xmax'] / width)\n",
    "        ymins.append(row['ymin'] / height)\n",
    "        ymaxs.append(row['ymax'] / height)\n",
    "        classes_text.append(row['class'].encode('utf8'))\n",
    "        classes.append(class_text_to_int(row['class']))\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes)\n",
    "    }))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "None has type NoneType, but expected one of: int, long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b2668dc6e286>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'filename'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrouped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtf_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tf_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_example\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e768d74fd99d>\u001b[0m in \u001b[0;36mcreate_tf_example\u001b[0;34m(group, path)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;34m'image/object/bbox/ymax'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataset_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat_list_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymaxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;34m'image/object/class/text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataset_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbytes_list_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;34m'image/object/class/label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataset_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64_list_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     }))\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/TensorFlow/object-detection-template/object_detection/utils/dataset_util.py\u001b[0m in \u001b[0;36mint64_list_feature\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mint64_list_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint64_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt64List\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: None has type NoneType, but expected one of: int, long"
     ]
    }
   ],
   "source": [
    "# Prepare the TFRecord Output\n",
    "TRAIN_RECORD_FILE = os.path.join(OUTPUT, 'train.tfrecord')\n",
    "EVAL_RECORD_FILE = os.path.join(OUTPUT, 'eval.tfrecord')\n",
    "\n",
    "# TRAIN TFRecord\n",
    "writer = tf.python_io.TFRecordWriter(path=TRAIN_RECORD_FILE)\n",
    "path_to_images = os.path.join(os.getcwd(), IMAGE_DIR)\n",
    "\n",
    "# From CSV to TFRecord\n",
    "grouped = split(train_df, 'filename')\n",
    "for group in tqdm(grouped):\n",
    "    tf_example = create_tf_example(group, path_to_images)\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "writer.close()\n",
    "\n",
    "# EVAL TFRecord\n",
    "writer = tf.python_io.TFRecordWriter(path=EVAL_RECORD_FILE)\n",
    "path_to_images = os.path.join(os.getcwd(), IMAGE_DIR)\n",
    "\n",
    "# From CSV to TFRecord\n",
    "grouped = split(eval_df, 'filename')\n",
    "for group in tqdm(grouped):\n",
    "    tf_example = create_tf_example(group, path_to_images)\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "writer.close()\n",
    "\n",
    "print('Successfully created the TFRecords: {}'.format(OUTPUT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have now converted the dataset into TFRecord format! In the next step we will train a pretrained model for finding Syd. Let's jump into `1_training` notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
